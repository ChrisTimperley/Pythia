#!/usr/bin/python3
from timeit import default_timer as timer
from subprocess import Popen, PIPE, TimeoutExpired
from pprint import pprint
import signal
import re
import types
import argparse
import os.path
import json
import shutil
import subprocess
import sys
import tempfile
import resource

MEM_LIMIT = 500 * (1000000) # 500 MB
INPUT_REGEX = r'(?<=\<\<SANDBOX>>\/)[\w|_|\.|-|\/]+\b'

# A special object used to indicate that a test execution timed out
class TestTimeout(object):
    def pretty(self):
        return "Timed out!"

# This function determines what the time limit should be for a test
# execution, given the duration of the oracle test execution and a flag
# indicating whether the execution is being used to generate coverage
# information.
def time_limit(duration, coverage_enabled):
    if coverage_enabled or duration < 1.0:
        return max(10.0 * duration, 2.0)
    elif duration > 10.0:
        multi = 2.0
    else:
        multi = 5.0

# Describes the state of the sandbox as a dictionary of file names and their
# associated SHA1 hashes.
def sandbox_state(d):
    cmd = "find '%s' -type f -exec sha1sum '{}' \;" % d
    state = subprocess.check_output(cmd, shell=True)
    state = state.decode(sys.stdout.encoding)
    state = [] if state == "" else state.splitlines(True)
    state = map(lambda l: l.split(' ', 1), state)
    state = {(f[len(d)+2:].strip()): h for (h, f) in state} # trim sandbox dir
    return state

# Provides a detailed description of a particular test suite
class TestManifest(object):
    # loads a manifest from a given file
    def __init__(self, fn):
        assert os.path.isfile(fn), "specified manifest file must exist"
        assert fn[-5:] == '.json', "specified manifest file must end in .json"
        with open(fn, 'r') as f:
            cases  = json.load(f)
            assert isinstance(cases, list), "manifest file must contain a JSON list"
            cases = [TestCase.from_json(i, c) for (i, c) in enumerate(cases)]
            self.__cases = cases

    def contents(self):
        return self.__cases
    def get(self, num):
        return self.__cases[num]

# Provides a particular test suite mapping
class TestMapping(object):
    def __init__(self, fn):
        assert os.path.isfile(fn), "specified mapping file must exist"
        assert fn[-5:] == '.json', "specified mapping file must end in .json"
        with open(fn, 'r') as f:
            self.__mapping = json.load(f)
    def mapping(self):
        return self.__mapping
    def get(self, id):
        return self.__mapping[id]

class TestInput(object):
    def __init__(self, maps_to, maps_from):
        self.__maps_to = maps_to
        self.__maps_from = maps_from
    def maps_to(self):
        return self.__maps_to
    def maps_from(self):
        return self.__maps_from

def preexecute():
    os.setsid()
    resource.setrlimit(resource.RLIMIT_AS, MEM_LIMIT)

class TestCase(object):
    @staticmethod
    def from_json(num, jsn):
        assert 'command' in jsn, "test case definition must specify 'command'"
        inpts = [TestInput(t, f) for (t, f) in jsn.get('input', {}).items()]
        return TestCase(num, jsn['command'], inpts)
    def __init__(self, num, command, inpts):
        self.__num = num
        self.__command = command
        self.__inpts = inpts
    def number(self):
        return self.__num
    def command(self):
        return self.__command
    def execute(self, executable_fn, inputd, tlim):
        assert tlim is None or tlim > 0
        # generate a sandbox directory for this test execution
        sandboxd = tempfile.mkdtemp()

        # calculate the absolute path for the executable
        executable_fn = os.path.abspath(executable_fn)

        # execute the test case within the sandbox, then ensure it's destroyed
        try:
            cmd = self.__command.replace("<<EXECUTABLE>>", executable_fn)
            cmd = cmd.replace("<<SANDBOX>>", "./")

            # prepare the inputs
            # TODO: for now the inputs are copied into the sandbox; in the
            #   future, we should allow the creation of symbolic links (when
            #   specified by the user).
            for inpt in self.__inpts:
                cp_from = os.path.join(inputd, inpt.maps_from())

                # if the file doesn't exist within the inputs directory, don't
                # try to copy it
                if not os.path.exists(cp_from):
                    continue

                cp_to = os.path.join(sandboxd, inpt.maps_to())
                cp_to_dir = os.path.dirname(cp_to)
                if not os.path.exists(cp_to_dir):
                    os.makedirs(cp_to_dir)
                shutil.copy2(cp_from, cp_to)

            # execute the command within the sandbox under the given time limit
            #
            # Credit to J.F. Sebastian on StackOverflow for advice on enforcing timeouts
            # and killing process groups when shell=True is used.
            #
            # http://stackoverflow.com/questions/36952245/subprocess-timeout-failure
            with Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE, preexec_fn=preexecute, cwd=sandboxd) as p:
                try:
                    t_start = timer()
                    stdout, stderr = p.communicate(timeout=tlim)
                    t_end = timer()
                    stdout = str(stdout)[2:-1]
                    stderr = str(stderr)[2:-1]
                    retcode = p.returncode
                    state = sandbox_state(sandboxd)
                    return TestOutcome(stdout, stderr, retcode, state, (t_end - t_start))
                # if the command timed out, return a special TestTimeout object
                except TimeoutExpired:
                    os.killpg(p.pid, signal.SIGKILL)
                    return TestTimeout()

        # ensure the sandbox is destroyed after execution
        finally:
            if os.path.exists(sandboxd):
                shutil.rmtree(sandboxd)

# Describes the outcome of a test case execution in terms of the standard
# output, standard error, return code, and state of the sandbox.
class TestOutcome(object):
    @staticmethod
    def from_json(jsn):
        return TestOutcome(jsn['out'], jsn['err'], jsn['retcode'], jsn['sandbox'], float(jsn['duration']))
    def __init__(self, stdout, stderr, retcode, sandbox, duration):
        self.__stdout = stdout
        self.__stderr = stderr
        self.__retcode = retcode
        self.__sandbox = sandbox
        self.__duration = float(duration)
    def stdout(self):
        return self.__stdout
    def stderr(self):
        return self.__stderr
    def retcode(self):
        return self.__retcode
    def sandbox(self):
        return self.__sandbox
    def duration(self):
        return self.__duration
    def __eq__(self, other):
        return not (other is None) and \
            not isinstance(other, TestTimeout) and \
            self.__stdout == other.stdout() and \
            self.__stderr == other.stderr() and \
            self.__retcode == other.retcode() and \
            self.__sandbox == other.sandbox()
    def pretty(self):
        pprint(self.to_json())
    def to_json(self):
        return {'out': self.__stdout,\
                'err': self.__stderr,\
                'duration': self.__duration,\
                'retcode': self.__retcode,\
                'sandbox': self.__sandbox}

# Defines the intended behaviour for a program on a given test suite
class Oracle(object):
    @staticmethod
    def generate(manifest, executable_fn, input_d, oracle_fn):
        assert isinstance(manifest, TestManifest), "manifest should be provided as a TestManifest object"
        assert not os.path.exists(oracle_fn), "oracle file must not already exist"
        assert oracle_fn[-5:] == '.json', "oracle file must end in '.json'"

        # compute the expected outcomes for each test
        outcomes = [case.execute(executable_fn, input_d, None)\
                    for case in manifest.contents()]
        oracle = Oracle(outcomes)

        # write the outcomes to the specified file, ensuring the file is
        # destroyed in the event of an exception (preventing a corrupted
        # oracle).
        try:
            with open(oracle_fn, 'w') as f:
                json.dump(oracle.to_json(), f)
        except:
            if os.path.exists(oracle_fn):
                os.remove(oracle_fn)
            raise

        return oracle

    # Attempts to load an oracle from a given file
    @staticmethod
    def load(oracle_fn):
        assert os.path.isfile(oracle_fn), "oracle file must exist"
        assert oracle_fn[-5:] == '.json', "oracle file must end in '.json'"
        with open(oracle_fn, 'r') as f:
            return Oracle([TestOutcome.from_json(o) for o in json.load(f)])

    def __init__(self, outcomes):
        self.__outcomes = outcomes
    
    # returns the expected outcome for a given test case
    # TODO: test case isn't in oracle
    def expected(self, test):
        return self.__outcomes[test.number()]

    def to_json(self):
        return [o.to_json() for o in self.__outcomes]

def run_test(manifest, oracle, executable, inputs, test, coverage_enabled):
    expected = oracle.expected(test)
    tlim = time_limit(expected.duration(), coverage_enabled)
    outcome = test.execute(executable, inputs, tlim)
    passed = outcome == expected

    print("Expected:")
    expected.pretty()
    print("\nActual:")
    outcome.pretty()
    print("")

    if passed:
        print("Finished running test case: PASSED")
        exit(0)
    else:
        print("Finished running test case: FAILED")
        exit(1)

# Generates the oracle for a given problem, storing its knowledge to disk at a
# specified oracle directory
def action_generate(args):
    assert os.path.isfile(args.executable), "specified executable must exist"
    assert os.path.isdir(args.inputs), "specified input directory must exist"

    manifest = TestManifest(args.tests)
    print("Generating oracle...")
    Oracle.generate(manifest, args.executable, args.inputs, args.output)
    print("Finished.\nSaved to disk at: %s" % args.output)

# Runs a test case with a given number against the oracle
def action_run(args):
    manifest = TestManifest(args.tests)
    oracle = Oracle.load(args.oracle)
    test = manifest.get(args.num)
    print("Running test case %d: %s" % (args.num, test.command()))
    return run_test(manifest, oracle, args.executable, args.inputs, test, args.coverage)

# Runs a test case with a given ID, supplied by the mapping file, against the
# oracle
def action_run_by_id(args):
    manifest = TestManifest(args.tests)
    mapping = TestMapping(args.mapping)
    oracle = Oracle.load(args.oracle)
    test_num = mapping.get(args.id)
    test = manifest.get(test_num)
    print("Running test case %s: %s" % (args.id, test.command()))
    return run_test(manifest, oracle, args.executable, args.inputs, test, args.coverage)

# Constructs a test manifest for a given problem by converting its MTS output
def action_build_mts(args):
    # generate mts command list
    try:
        subprocess.check_call(["mts",\
                               args.object, args.executable, args.universe,\
                               "R",\
                               "commands.txt", "NULL", "NULL"])
        print("Generated MTS output")
        subprocess.check_call(("grep -e '^%s' commands.txt | sponge commands.txt" % args.executable),\
                              shell=True)
        # WARN:
        #   really, the matching portion of this should be: ${object}/inputs
        #   however, MTS seems to produce "../inputs", regardless of where the
        #   object directory may be
        subprocess.check_call("sed -i 's;../inputs;<<SANDBOX>>;g' commands.txt",\
                              shell=True)
        subprocess.check_call(("sed -i 's; > %s/outputs/.\+$;;g' commands.txt" % args.object),\
                              shell=True)
        subprocess.check_call(("sed -i 's;%s;<<EXECUTABLE>>;g' commands.txt" % args.executable),\
                              shell=True)
        print("Sanitised MTS output into list of Pythia commands")
    
    # destroy commands.txt in the event of an error
    except:
        if os.path.exists("commands.txt"):
            os.remove("commands.txt")
        raise

    # convert the commands list into a JSON Pythia manifest
    manifest = []
    with open("commands.txt", "r") as f:
        for cmd in f:
            cmd = cmd.strip()
            inpts = {}
            for inpt in re.findall(INPUT_REGEX, cmd):
                inpts[inpt] = inpt
            manifest.append({'command': cmd, 'input': inpts})

    # write the manifest to file
    with open(args.output, "w") as f:
        json.dump(manifest, f, indent=2, sort_keys=True)

# Determines the passing and failing tests for a variant of a program against
# the oracle. These results are used to generate a mapping between GenProg
# style test identifiers and their numbers in the test manifest.
def action_map(args):
    assert not os.path.exists("map.pythia.json"), "map.pythia.json must not exist within working directory"
    manifest = TestManifest(args.tests)
    oracle = Oracle.load(args.oracle)

    print("Generating test case mapping...")
    m = {}
    failed = []
    num_passed = 0
    num_failed = 0
    for test in manifest.contents():
        expected = oracle.expected(test)
        tlim = time_limit(expected.duration(), False)
        actual = test.execute(args.executable, args.inputs, tlim)
        outcome = actual == expected
        if outcome:
            num_passed += 1
            m["p%d" % num_passed] = test.number()
        else:
            num_failed += 1
            failed.append(str(test.number()))
            m["n%d" % num_failed] = test.number()

    # debugging
    print("Generated test case mapping")
    print("Found %d failing tests: %s" % (num_failed, ', '.join(failed)))
    print("Saving map file to disk at: map.pythia.json")
    with open('map.pythia.json', 'w') as f:
        json.dump(m, f)
    print("Saved map file to: map.pythia.json")

# CLI setup
PARSER = argparse.ArgumentParser()
SUBPARSERS = PARSER.add_subparsers()
PARSER.add_argument('--version', action='version', version='0.0.1')

# generate action
GENERATE_PARSER = SUBPARSERS.add_parser('generate')
GENERATE_PARSER.add_argument('executable',\
                             help='location of program executable')
GENERATE_PARSER.add_argument('--inputs',\
                             help='location of test case inputs directory',\
                             default='inputs')
GENERATE_PARSER.add_argument('-t', '--tests',\
                             help='location of test suite manifest file',\
                             default='tests.pythia.json')
GENERATE_PARSER.add_argument('-o', '--output',\
                             help='file to save oracle at',\
                             default='oracle.pythia.json')
GENERATE_PARSER.set_defaults(func=action_generate)

# convert-mts action
BUILD_MTS_PARSER = SUBPARSERS.add_parser('build-manifest-from-mts')
BUILD_MTS_PARSER.add_argument('object',\
                              help='location of root, object directory')
BUILD_MTS_PARSER.add_argument('universe',\
                              help='location of TSL universe file')
BUILD_MTS_PARSER.add_argument('executable',\
                             help='location of program executable')
BUILD_MTS_PARSER.add_argument('-o', '--output',\
                             help='file to save the converted test manifest at',\
                             default='tests.pythia.json')
BUILD_MTS_PARSER.set_defaults(func=action_build_mts)

# run action
RUN_PARSER = SUBPARSERS.add_parser('run')
RUN_PARSER.add_argument('executable',\
                        help='location of program executable')
RUN_PARSER.add_argument('num',\
                        type=int,\
                        help='number of the test case that should be executed')
RUN_PARSER.add_argument('--inputs',\
                        help='location of test case inputs directory',\
                        default='inputs')
RUN_PARSER.add_argument('--oracle',\
                        help='location of oracle file, used for validation',\
                        default='oracle.pythia.json')
RUN_PARSER.add_argument('-t', '--tests',\
                        help='location of test suite manifest file',\
                        default='tests.pythia.json')
RUN_PARSER.add_argument('--coverage',\
                        action='store_true',\
                        help='flag indicating whether coverage is enabled',\
                        default=False)
RUN_PARSER.set_defaults(func=action_run)

# run by id action
RUN_ID_PARSER = SUBPARSERS.add_parser('run-by-id')
RUN_ID_PARSER.add_argument('executable',\
                        help='location of program executable')
RUN_ID_PARSER.add_argument('id',\
                        help='id of the test that should be run')
RUN_ID_PARSER.add_argument('--inputs',\
                        help='location of test case inputs directory',\
                        default='inputs')
RUN_ID_PARSER.add_argument('--oracle',\
                        help='location of oracle file, used for validation',\
                        default='oracle.pythia.json')
RUN_ID_PARSER.add_argument('-t', '--tests',\
                        help='location of test suite manifest file',\
                        default='tests.pythia.json')
RUN_ID_PARSER.add_argument('--mapping',\
                        help='location of test mapping file',\
                        default='map.pythia.json')
RUN_ID_PARSER.add_argument('--coverage',\
                        action='store_true',\
                        help='flag indicating whether coverage is enabled',\
                        default=False)
RUN_ID_PARSER.set_defaults(func=action_run_by_id)

# map action
MAP_PARSER = SUBPARSERS.add_parser('map')
MAP_PARSER.add_argument('executable',\
                        help='location of program executable')
MAP_PARSER.add_argument('--inputs',\
                        help='location of test case inputs directory',\
                        default='inputs')
MAP_PARSER.add_argument('--oracle',\
                        help='location of oracle file, used for validation',\
                        default='oracle.pythia.json')
MAP_PARSER.add_argument('-t', '--tests',\
                        help='location of test suite manifest file',\
                        default='tests.pythia.json')
MAP_PARSER.set_defaults(func=action_map)

def main():
    args = PARSER.parse_args()
    if 'func' in vars(args):
        args.func(args)
